{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most harmless user function of Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1: \n",
      "    A  B\n",
      "0  1  1\n",
      "1  2  2\n",
      "2  3  6 \n",
      "\n",
      "df1: \n",
      "    B   C\n",
      "0  1  10\n",
      "1  2  11\n",
      "2  9  12 \n",
      "\n",
      "df1: \n",
      "     C   D\n",
      "0  10  16\n",
      "1   3  17\n",
      "2  15  18 \n",
      "\n",
      "merged df: \n",
      "      A    B   C     D\n",
      "0  1.0  1.0  10  16.0\n",
      "1  2.0  2.0  11   NaN\n",
      "2  NaN  NaN   3  17.0\n",
      "3  NaN  NaN  15  18.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import pandas as pd\n",
    "def merge_df_list(\n",
    "    df_left: pd.DataFrame, \n",
    "    dfs_right: List[pd.DataFrame], \n",
    "    keys: List[str], \n",
    "    methods: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    功能：横向合并多个df\n",
    "    参数：\n",
    "    df_left: 最左边的df\n",
    "    df_list：需要合并的df列表(除最左边的以外)\n",
    "    keys：合并df所需要的key列表，需要与df_list一一对应, \n",
    "        列表元素为一个二元元组，元组元素为str列表\n",
    "    methods: 合并df所需要的方法列表，需要与df_list一一对应\n",
    "\n",
    "    返回值：合并后的df\n",
    "    \"\"\"\n",
    "    # 将最左边的数据帧赋值给df_merged\n",
    "    df_merged = df_left\n",
    "    # 使用zip函数同时迭代df_right，keys和methods列表\n",
    "    for df, key, method in zip(dfs_right, keys, methods):\n",
    "        # 使用指定的键和方法合并当前数据帧与df_merged\n",
    "        df_merged = df_merged.merge(df, left_on=key[0], \n",
    "                        right_on=key[1], how=method)\n",
    "    # 返回合并后的数据帧\n",
    "    return df_merged\n",
    "\n",
    "# 示例代码：\n",
    "if __name__ == \"__main__\":\n",
    "    df1 = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [1, 2, 6]})\n",
    "    df2 = pd.DataFrame({\"B\": [1, 2, 9], \"C\": [10, 11, 12]})\n",
    "    df3 = pd.DataFrame({\"C\": [10, 3, 15], \"D\": [16, 17, 18]})\n",
    "\n",
    "\n",
    "    df_left = df1\n",
    "    df_right = [df2, df3]\n",
    "    keys = [(\"B\",\"B\"),(\"C\",\"C\")]\n",
    "    methods = ['inner','outer']\n",
    "\n",
    "    merged_df = merge_df_list(df_left,df_right, keys,methods)\n",
    "    print(f\"df1: \\n {df1} \\n\")\n",
    "    print(f\"df1: \\n {df2} \\n\")\n",
    "    print(f\"df1: \\n {df3} \\n\")\n",
    "    print(f\"merged df: \\n {merged_df} \\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量扫描文件夹并获取文件路径\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未排除csv文件的路径如下: \n",
      " ./assets/auto.dta \n",
      "\n",
      "未排除csv文件的路径如下: \n",
      " ./assets/auto.csv \n",
      "\n",
      "排除csv文件的路径如下: \n",
      " ./assets/auto.dta \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "from typing import List, Generator\n",
    "\n",
    "def scan_file_path(\n",
    "    folder: str, \n",
    "    extensions: List[str], \n",
    "    exclude: str=\"^$\", \n",
    "    recursive: bool=False) -> Generator[str, None, None]:\n",
    "    \"\"\"生成器函数，用于生成符合指定条件的文件路径。\n",
    "\n",
    "    Args:\n",
    "        folder (str): 要扫描的文件夹。\n",
    "        extensions (List[str]): 要匹配的文件扩展名列表。\n",
    "        exclude (str, optional): 要排除的通配符模式。默认为 \"^$\"(完全不排除)。\n",
    "        recursive (bool, optional): 是否递归到子目录。默认为 False。\n",
    "\n",
    "    Yields:\n",
    "        str: 文件路径。\n",
    "    \"\"\"\n",
    "    # 验证输入文件夹\n",
    "    if not os.path.isdir(folder):\n",
    "        raise ValueError(\"'{}' 不是存在的文件夹\".format(folder))\n",
    "    # 使用 os.scandir 遍历文件夹中的条目\n",
    "    with os.scandir(folder) as it:\n",
    "        for entry in it:\n",
    "            # 检查条目是否为文件，并且其名称是否与扩展名和排除模式匹配\n",
    "            if (entry.is_file() \n",
    "                and entry.name.endswith(tuple(extensions)) \n",
    "                and not re.search(exclude, entry.path)):\n",
    "                # 输出文件路径\n",
    "                yield entry.path\n",
    "            # 检查条目是否为目录，并且是否启用递归扫描\n",
    "            elif entry.is_dir() and recursive:\n",
    "                # 递归到子目录\n",
    "                yield from scan_file_path(entry.path, extensions, exclude, recursive)\n",
    "\n",
    "\n",
    "\n",
    "# 示例代码\n",
    "if __name__ == \"__main__\":\n",
    "  extensions = ['.csv','.dta']\n",
    "  folder = \".\" \n",
    "  exclude = r\".csv$\"\n",
    "  paths1 = scan_file_path(folder,extensions,recursive=True)\n",
    "  paths2 = scan_file_path(folder,extensions,recursive=True,exclude=exclude)\n",
    "\n",
    "  for path in paths1:\n",
    "    print(f\"未排除csv文件的路径如下: \\n {path} \\n\")\n",
    "\n",
    "  for path in paths2:\n",
    "    print(f\"排除csv文件的路径如下: \\n {path} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'c': 3} \n",
      "\n",
      "     value\n",
      "key       \n",
      "a        1\n",
      "b        2\n",
      "c        3 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Any\n",
    "def dict_to_df(_dict: Dict[Any, Any], key_name: str, value_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    字典转换为一个dataframe, 字典键对应第一列，字典值第二列。\n",
    "\n",
    "    参数：\n",
    "    _dict (Dict[Any, Any]): 字典。\n",
    "    key_name (str): 字典的keys对应的列名。\n",
    "    value_name (str): 字典的values对应的列名。\n",
    "    \n",
    "    返回值：\n",
    "    df(pd.DataFrame): 一个两列dataframe，第一列对应字典的keys，第二列对应字典的values。\n",
    "    \"\"\"\n",
    "    df = (pd.DataFrame.from_dict(_dict, orient='index', columns=[value_name])\n",
    "        .rename_axis(key_name))\n",
    "  \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 定义测试字典\n",
    "    test_dict = {'a': 1, 'b': 2, 'c': 3}\n",
    "\n",
    "    # 调用 dict_to_df 函数\n",
    "    df = dict_to_df(test_dict, 'key', 'value')\n",
    "\n",
    "    # 打印输出结果，查看是否符合预期\n",
    "    print(f\"{test_dict} \\n\")\n",
    "    print(f\"{df} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成器data1中的df块1:\n",
      "   _column_name       _column_label _value_label_name _value_label  \\\n",
      "0         make      Make and model               nan          nan   \n",
      "1        price               Price               nan          nan   \n",
      "2          mpg       Mileage (mpg)               nan          nan   \n",
      "3        rep78  Repair record 1978               nan          nan   \n",
      "4     headroom      Headroom (in.)               nan          nan   \n",
      "\n",
      "            make price mpg rep78 headroom trunk weight length turn  \\\n",
      "0    AMC Concord  4099  22     3      2.5    11   2930    186   40   \n",
      "1      AMC Pacer  4749  17     3      3.0    11   3350    173   40   \n",
      "2     AMC Spirit  3799  22     .      3.0    12   2640    168   35   \n",
      "3  Buick Century  4816  20     3      4.5    16   3250    196   40   \n",
      "4  Buick Electra  7827  15     4      4.0    20   4080    222   43   \n",
      "\n",
      "  displacement          gear_ratio foreign  \n",
      "0          121  3.5799999237060547       0  \n",
      "1          258  2.5299999713897705       0  \n",
      "2          121  3.0799999237060547       0  \n",
      "3          196   2.930000066757202       0  \n",
      "4          350  2.4100000858306885       0  \n",
      "_column_name         object\n",
      "_column_label        object\n",
      "_value_label_name    object\n",
      "_value_label         object\n",
      "make                 object\n",
      "price                object\n",
      "mpg                  object\n",
      "rep78                object\n",
      "headroom             object\n",
      "trunk                object\n",
      "weight               object\n",
      "length               object\n",
      "turn                 object\n",
      "displacement         object\n",
      "gear_ratio           object\n",
      "foreign              object\n",
      "dtype: object \n",
      "\n",
      "生成器data1中的df块2:\n",
      "   _column_name       _column_label _value_label_name _value_label make price  \\\n",
      "0         make      Make and model               nan          nan  nan   nan   \n",
      "1        price               Price               nan          nan  nan   nan   \n",
      "2          mpg       Mileage (mpg)               nan          nan  nan   nan   \n",
      "3        rep78  Repair record 1978               nan          nan  nan   nan   \n",
      "4     headroom      Headroom (in.)               nan          nan  nan   nan   \n",
      "\n",
      "   mpg rep78 headroom trunk weight length turn displacement gear_ratio foreign  \n",
      "0  nan   nan      nan   nan    nan    nan  nan          nan        nan     nan  \n",
      "1  nan   nan      nan   nan    nan    nan  nan          nan        nan     nan  \n",
      "2  nan   nan      nan   nan    nan    nan  nan          nan        nan     nan  \n",
      "3  nan   nan      nan   nan    nan    nan  nan          nan        nan     nan  \n",
      "4  nan   nan      nan   nan    nan    nan  nan          nan        nan     nan  \n",
      "_column_name         object\n",
      "_column_label        object\n",
      "_value_label_name    object\n",
      "_value_label         object\n",
      "make                 object\n",
      "price                object\n",
      "mpg                  object\n",
      "rep78                object\n",
      "headroom             object\n",
      "trunk                object\n",
      "weight               object\n",
      "length               object\n",
      "turn                 object\n",
      "displacement         object\n",
      "gear_ratio           object\n",
      "foreign              object\n",
      "dtype: object \n",
      "\n",
      "生成器data3中的df块1:\n",
      "             make  price  mpg rep78  headroom  trunk  weight  length  turn  \\\n",
      "0    AMC Concord   4099   22     3       2.5     11    2930     186    40   \n",
      "1      AMC Pacer   4749   17     3       3.0     11    3350     173    40   \n",
      "2     AMC Spirit   3799   22     .       3.0     12    2640     168    35   \n",
      "3  Buick Century   4816   20     3       4.5     16    3250     196    40   \n",
      "4  Buick Electra   7827   15     4       4.0     20    4080     222    43   \n",
      "\n",
      "   displacement  gear_ratio  foreign  \n",
      "0           121        3.58        0  \n",
      "1           258        2.53        0  \n",
      "2           121        3.08        0  \n",
      "3           196        2.93        0  \n",
      "4           350        2.41        0  \n",
      "生成器data3中的df块2:\n",
      "              make  price  mpg rep78  headroom  trunk  weight  length  turn  \\\n",
      "50  Pont. Phoenix   4424   19     .       3.5     13    3420     203    43   \n",
      "51  Pont. Sunbird   4172   24     2       2.0      7    2690     179    41   \n",
      "52      Audi 5000   9690   17     5       3.0     15    2830     189    37   \n",
      "53       Audi Fox   6295   23     3       2.5     11    2070     174    36   \n",
      "54       BMW 320i   9735   25     4       2.5     12    2650     177    34   \n",
      "\n",
      "    displacement  gear_ratio  foreign  \n",
      "50           231        3.08        0  \n",
      "51           151        2.73        0  \n",
      "52           131        3.20        1  \n",
      "53            97        3.70        1  \n",
      "54           121        3.64        1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Generator, List\n",
    "import numpy as np\n",
    "\n",
    "def read_stata(\n",
    "    file_path: str, \n",
    "    chunksize: int=None, \n",
    "    keep_data: str = 'all', \n",
    "    convert_categoricals:bool=False,\n",
    "    preserve_dtypes:bool=False,\n",
    "    convert_missing:bool=True,\n",
    "    usecols:List[str]|None=None,\n",
    "    dtype: List[str] | str | None=None) -> Generator:\n",
    "    \"\"\"\n",
    "    读取 Stata 文件并返回数据和标签。\n",
    "    \n",
    "    参数:\n",
    "    file_path (str): Stata 文件的路径。\n",
    "    chunksize (int): 读取stata文件的数据块的大小。\n",
    "    keep_data (str): 保留数据的类型, 'all'为数据和标签，'only_label'仅标签，\n",
    "                    'only_data'仅数据，默认为'all'。\n",
    "    convert_categoricals (bool): 是否转换原始值为值标签对应值，默认值为False。\n",
    "                    注意，有些文件转换会报错。\n",
    "    convert_missing (bool): 是否以stata缺失值类型存储，默认值为True。\n",
    "    usecols (List[str]|None): 保留的列，默认值None保留所有列。\n",
    "    dtype (List[str]||None): 制定列的数据类型，默认为None。\n",
    "\n",
    "    \n",
    "    返回值:\n",
    "    Generator: 返回一个(DataFrame)生成器。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 创建 StataReader 并设置参数。\n",
    "    reader = pd.read_stata(\n",
    "        file_path, \n",
    "        chunksize=chunksize, \n",
    "        convert_categoricals=convert_categoricals,\n",
    "        preserve_dtypes=preserve_dtypes,\n",
    "        convert_missing=convert_missing,\n",
    "        columns=usecols)\n",
    "    # 如果保留标签信息\n",
    "    if keep_data in ['all','only_label']:\n",
    "        # 获取Stata文件的变量标签dataframe\n",
    "        variable_labels = dict_to_df(\n",
    "            reader.variable_labels(),\n",
    "            key_name='_column_name',\n",
    "            value_name='_column_label').reset_index()\n",
    "        # 获取Stata文件的值标签dataframe\n",
    "        value_labels = dict_to_df(\n",
    "            reader.value_labels(),\n",
    "            key_name='_value_label_name',\n",
    "            value_name='_value_label').reset_index()\n",
    "        # Outer横向合并生成标签dataframe\n",
    "        label = pd.merge(\n",
    "            variable_labels,\n",
    "            value_labels,\n",
    "            left_on='_column_name',\n",
    "            right_on='_value_label_name',\n",
    "            how='outer',\n",
    "            copy=False).astype(str)\n",
    "\n",
    "        if keep_data == \"only_label\":\n",
    "            # 仅返回label信息\n",
    "            return label\n",
    "            \n",
    "        else: \n",
    "            # 返回包含标签和数据的信息\n",
    "            for df in reader:\n",
    "                labels = pd.concat([label,df],axis=1,join='outer')\n",
    "                if dtype:\n",
    "                    yield labels.astype(dtype)\n",
    "                else:\n",
    "                    yield labels\n",
    "    elif keep_data == \"only_data\":\n",
    "        # 仅返回数据数据dataframe块\n",
    "        for df in reader:\n",
    "            if dtype:\n",
    "                yield df.astype(dtype)\n",
    "            else:\n",
    "                yield df\n",
    "    \n",
    "    else:\n",
    "        # 返回错误\n",
    "        raise ValueError(f\"paramter 'keep_data' got an unexpected value '{keep_data}'\")\n",
    "\n",
    "\n",
    "# 函数调用示例：\n",
    "if __name__ == \"__main__\":\n",
    "    # 生成两个示例的df chunks生成器\n",
    "    data1 = read_stata(\"./assets/auto.dta\",\n",
    "            chunksize=50,dtype='str')\n",
    "    data2 = read_stata(\"./assets/auto.dta\",\n",
    "            chunksize=50,keep_data=\"only_label\")\n",
    "    data3 = read_stata(\"./assets/auto.dta\",chunksize=50,\n",
    "            keep_data=\"only_data\")\n",
    "   \n",
    "    # # 遍历生成器\n",
    "    for index, df in enumerate(data1):\n",
    "        print(f\"生成器data1中的df块{index+1}:\\n {df.head(5)}\")\n",
    "        print(df.dtypes, \"\\n\")\n",
    "    for index, df in enumerate(data2):\n",
    "        print(f\"生成器data2中的df块{index+1}:\\n {df.head(5)}\")\n",
    "    for index, df in enumerate(data3):\n",
    "        print(f\"生成器data3中的df块{index+1}:\\n {df.head(5)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['table1_1', 'table2_1', 'table3_1', 'table2', 'table3']\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "def get_table_names(db_file: str) -> List[str]:\n",
    "    \"\"\"获取给定数据库中的表名列表。\n",
    "\n",
    "    Args:\n",
    "        db_file: 数据库文件的路径。\n",
    "\n",
    "    Returns:\n",
    "        数据库中的表名列表。\n",
    "    \"\"\"\n",
    "    # 获取数据库中的所有表名\n",
    "    with sqlite3.connect(db_file) as conn:\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        table_names = [table[0] for table in cursor]\n",
    "\n",
    "    # 如果给定的表名在列表中，则返回 True，否则返回 False\n",
    "    return table_names\n",
    "\n",
    "# 示例调用\n",
    "print(get_table_names('my_database.db'))  # 输出：True 或 False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "def get_unique_name(name: str, name_list: List[str]) -> str:\n",
    "    \"\"\"获取唯一的名字。\n",
    "\n",
    "    如果给定的名字已经在列表中，则提示用户输入新的名字，直到输入的名字在列表中不存在为止。返回不存在于列表中的名字。\n",
    "\n",
    "    参数:\n",
    "    name: str 需要检查的名字。\n",
    "    name_list: List[str] 名字列表。\n",
    "\n",
    "    返回:\n",
    "    str: 不存在于列表中的名字。\n",
    "    \"\"\"\n",
    "    new_name = name\n",
    "    while new_name in name_list:\n",
    "        new_name = input(\"请输入一个新的名字: \")\n",
    "    return new_name\n",
    "\n",
    "def delete_table(table_name: str, db_file: str):\n",
    "    \"\"\"\n",
    "    在指定数据库文件中删除指定的表。\n",
    "\n",
    "    参数:\n",
    "    - table_name: 需要删除的表的名称。\n",
    "    - db_file: 数据库文件的路径。\n",
    "    \"\"\"\n",
    "    with sqlite3.connect(db_file) as conn:\n",
    "        c = conn.cursor()\n",
    "        c.execute(f\"DROP TABLE {table_name}\")\n",
    "\n",
    "def name_table(name: str, db_file: str, delete_existing_table: bool = False) -> str | None:\n",
    "    \"\"\"\n",
    "    为 SQLite 数据库表命名。如与原表名冲突，则选择要么重命名要么删除原表。\n",
    "\n",
    "    参数：\n",
    "    - name (str): 用户定义的名称。\n",
    "    - db_file (str): SQLite 数据库的文件路径。\n",
    "    - delete_existing_table (bool): 当表存在时，False（默认）为用户输入新名，True 为删除原表。\n",
    "\n",
    "    返回值：\n",
    "    - str | None: 表名或 None。\n",
    "    \"\"\"\n",
    "    # 获取 SQLite 数据库中的全部表名\n",
    "    table_names = get_table_names(db_file)\n",
    "    \n",
    "    # 当命名与已有表名冲突时\n",
    "    if name in table_names:\n",
    "        # 删除原表或输入新名字\n",
    "        return delete_table(name, db_file) if delete_existing_table else get_unique_name(name, table_names)\n",
    "    else:\n",
    "        # 返回原名\n",
    "        return name\n",
    "\n",
    "\n",
    "# 函数调用示例：\n",
    "if __name__ == \"__main__\":\n",
    "    table_name = name_table('table1',\"my_database.db\",delete_existing_table=True)\n",
    "    print(table_name,'\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import os\n",
    "import pandas as pd\n",
    "from typing import Tuple, List, Generator\n",
    "def read_data(\n",
    "    filepath: str, \n",
    "    chunksize: int= None, \n",
    "    usecols: List[str]= None,\n",
    "    dtype:List[str] | str= 'str')->Generator[Tuple[str, pd.DataFrame],None,None]:\n",
    "    \"\"\"\n",
    "    读取文件数据\n",
    "    \n",
    "    参数:\n",
    "    filepath: str 文件路径。\n",
    "    chunksize: int, (optional) 块大小，用于指定读取数据文件时分块的大小。如果未指定，则不会将数据文件分块。\n",
    "    usecols: List[str], (optional) 列名列表，用于指定导入的列。如果未指定，则为全部列。\n",
    "    dtype: List[str] | str, (optional) 列数据类型或其列表，用于制定列的数据类型，默认全部为'str'。\n",
    "        \n",
    "    Yields:\n",
    "    Generator[Tuple[str, pd.DataFrame],None,None] 生成器，包含文件名和数据块的元组。\n",
    "    \"\"\"\n",
    "    # 获取文件名和文件扩展名\n",
    "    filename, file_extension = os.path.splitext(filepath)\n",
    "\n",
    "    # 定义将文件扩展名映射到读取函数的字典\n",
    "    extensions_to_readers = {\n",
    "        '.csv': pd.read_csv,\n",
    "        '.dta': read_stata,\n",
    "        '.xlsx': pd.read_excel\n",
    "    }\n",
    "\n",
    "    # 使用 lru_cache 装饰器缓存每种文件扩展名的读取函数的结果\n",
    "    @functools.lru_cache()\n",
    "    def reader(ext):\n",
    "        return extensions_to_readers.get(ext)\n",
    "\n",
    "    # 从缓存中查找读取函数\n",
    "    reader_fn = reader(file_extension)\n",
    "\n",
    "    # 检查文件扩展名是否受支持\n",
    "    if reader_fn is None:\n",
    "        raise ValueError(f\"Unsupported file extension: {file_extension}\")\n",
    "\n",
    "    # 使用指定的块大小以块的方式读取数据文件\n",
    "    for chunk in reader_fn(filepath, chunksize=chunksize, iterator=True):\n",
    "        # 将文件名和 DataFrame 块作为生成器的一部分生成\n",
    "        yield filename, chunk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Iterator\n",
    "\n",
    "def write_df_to_sqlite(\n",
    "    df: Union[pd.DataFrame, Iterator[pd.DataFrame]],\n",
    "    db_file: str, \n",
    "    table_name: str, \n",
    "    chunk_size: Union[int, None]=None) -> None:\n",
    "  # 连接数据库\n",
    "  with sqlite3.connect(db_file) as conn:\n",
    "    # 将数据帧按块写入数据库表中\n",
    "    chunk_iter = df if isinstance(df, Iterator) else df.groupby(np.arange(len(df)) // chunk_size) \\\n",
    "        if chunk_size else [df]\n",
    "    if_exists = 'replace' if isinstance(df, pd.DataFrame) else 'append'\n",
    "    for chunk in chunk_iter:\n",
    "      # 获取数据帧块\n",
    "      chunk_df = chunk if isinstance(df, Iterator) else chunk[1]\n",
    "      # 将数据帧块写入数据库表中\n",
    "      chunk_df.to_sql(table_name, conn, if_exists=if_exists)\n",
    "      # 提交事务\n",
    "      conn.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data import DataCleaner\n",
    "import pandas as pd\n",
    "\n",
    "folder = '/mnt/Data/data/CFPS/CFPS_tidy/csv' \n",
    "dc= DataCleaner()\n",
    "\n",
    "paths = dc.scan_file_path(folder)\n",
    "\n",
    "dc.import_csv_to_sqlite(paths,'test1.db')\n",
    "# datas =  dc.read_data(paths)\n",
    "\n",
    "# dc.write_to_sqlite(datas,'test.db')\n",
    "\n",
    "\n",
    "\n",
    "# for path in data_paths:\n",
    "#     print(path)\n",
    "# datas = dc.read_data(paths)\n",
    "\n",
    "# for name, df in datas:\n",
    "#     print(name)\n",
    "#     display(df)\n",
    "# dc.write_to_db(datas,'test.db')\n",
    "\n",
    "# for path in paths:\n",
    "#     print(path)\n",
    "\n",
    "\n",
    "# for chunk in dc._read_stata('/mnt/Data/data/CFPS/CFPS_tidy/child2012.dta'):\n",
    "#     print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data import DataCleaner\n",
    "\n",
    "\n",
    "folder = '/mnt/Data/data/CFPS/CFPS_tidy/csv' \n",
    "ldc= LightDataCleaner()\n",
    "\n",
    "paths = ldc.scan_csv_path(folder)\n",
    "\n",
    "ldc.import_csv_to_sqlite(paths,'cfps.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Data import DataCleaner\n",
    "\n",
    "dc = DataCleaner()\n",
    "data = dc._read_stata('/mnt/Data/data/CFPS/CFPS_tidy/child2010_v201906.dta',keep_data='only_label')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
